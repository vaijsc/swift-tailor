{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.mics import get_all_gcd_files\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def meshing_uv_map(occupancy):\n",
    "    occ = occupancy.astype(bool)\n",
    "    pixel_index = np.arange(occ.size).reshape(occ.shape)\n",
    "\n",
    "    # Determine triangles' vertices\n",
    "    is_tri_vert = (\n",
    "        occ & np.roll(occ, shift=-1, axis=0) & np.roll(occ, shift=-1, axis=1)\n",
    "    )\n",
    "    verta = pixel_index\n",
    "    vertb = np.roll(pixel_index, shift=-1, axis=1)\n",
    "    vertc = np.roll(pixel_index, shift=-1, axis=0)\n",
    "    face0 = np.stack(\n",
    "        [verta[is_tri_vert], vertb[is_tri_vert], vertc[is_tri_vert]], axis=1\n",
    "    )\n",
    "\n",
    "    # Determine the second set of triangles' vertices\n",
    "    is_tri_vert = (\n",
    "        occ & np.roll(occ, shift=1, axis=0) & np.roll(occ, shift=1, axis=1)\n",
    "    )\n",
    "    verta = pixel_index\n",
    "    vertb = np.roll(pixel_index, shift=1, axis=1)\n",
    "    vertc = np.roll(pixel_index, shift=1, axis=0)\n",
    "    face1 = np.stack(\n",
    "        [verta[is_tri_vert], vertb[is_tri_vert], vertc[is_tri_vert]], axis=1\n",
    "    )\n",
    "\n",
    "    # Combine the two sets of faces\n",
    "    face = np.concatenate([face0, face1], axis=0)\n",
    "\n",
    "    return face\n",
    "\n",
    "\n",
    "all_files = get_all_gcd_files()\n",
    "for fi in all_files:\n",
    "    omg_path = Path(fi) / f\"{Path(fi).stem}_gim.npz\"\n",
    "    try:\n",
    "        omg = np.load(omg_path)['arr_0']\n",
    "    except:\n",
    "        continue\n",
    "    occupancy = np.any(omg > 0, axis=-1)\n",
    "    faces = meshing_uv_map(occupancy)\n",
    "\n",
    "    a = np.sum(occupancy.astype(int))\n",
    "    b = np.unique(faces.flatten()).shape[0]\n",
    "    if a != b:\n",
    "        print(a, b, a - b, fi)\n",
    "        # break\n",
    "\n",
    "\n",
    "def create_obj(omg: np.ndarray, faces: np.ndarray):\n",
    "    omg_size = omg.shape[0]\n",
    "\n",
    "    idx = np.unique(faces.flatten())\n",
    "    new_idx = {\n",
    "        old: new + 1 for new, old in enumerate(idx)\n",
    "    }\n",
    "    list_vert = []\n",
    "    for i in idx:\n",
    "        list_vert.append(omg[(i // omg_size), (i % omg_size)])\n",
    "\n",
    "    with open(\"output.obj\", \"w\") as f:\n",
    "        for vert in list_vert:\n",
    "            f.write(f\"v {vert[0]} {vert[1]} {vert[2]}\\n\")\n",
    "\n",
    "        for face in faces:\n",
    "            f.write(f\"f {new_idx[face[0]]} {new_idx[face[1]]} {new_idx[face[2]]}\\n\")\n",
    "\n",
    "\n",
    "# create_obj(omg, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 111 points\n",
    "# omg_path = \"/home/phphuc/Desktop/GarmentRecon/GarmentCode/data/GarmentCodeData_v2/GarmentCodeData_v2/garments_5000_0/default_body/data/rand_H1HW5UW5S7/rand_H1HW5UW5S7_gim.npz\"\n",
    "\n",
    "omg_path = \"/home/phphuc/Desktop/GarmentRecon/GarmentCode/data/GarmentCodeData_v2/GarmentCodeData_v2/garments_5000_0/default_body/data/rand_2E7LYE4R51/rand_2E7LYE4R51_gim.npz\"\n",
    "omg = np.load(omg_path)['arr_0']\n",
    "\n",
    "occupancy = np.any(omg > 0, axis=-1)\n",
    "faces = meshing_uv_map(occupancy)\n",
    "create_obj(omg, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(faces.flatten()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2omg(omg_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Define the model repository\n",
    "model_name = \"black-forest-labs/FLUX.1-dev-onnx\"\n",
    "\n",
    "# Download the model\n",
    "local_model_path = snapshot_download(repo_id=model_name)\n",
    "\n",
    "print(f\"Model downloaded to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"black-forest-labs/FLUX.1-dev-onnx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load ONNX runtime session\n",
    "onnx_model_path = f\"{model_name}/model.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Example text input\n",
    "text = \"This is a test input.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "\n",
    "# Prepare inputs for ONNX\n",
    "onnx_inputs = {k: v.astype(np.int64) for k, v in inputs.items()}\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, onnx_inputs)\n",
    "\n",
    "# Print results\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
